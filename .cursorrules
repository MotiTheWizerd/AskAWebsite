# Cursor Rules for Crawl4AI Project

## Project Description

This project is designed to scrape website content using the `Crawl4AI` framework. Users will input a website URL, and the application will scrape the website's content and allow them to query an LLM (Language Learning Model) about the scraped data. The project will follow Python best practices and maintain industry-standard code separation for scalability and maintainability.

---

## General Project Rules

1. **Follow Python Best Practices**

   - Adhere to `PEP 8` for code formatting, including indentation, line length, and naming conventions.
   - Use `snake_case` for variables and functions, and `PascalCase` for classes.

2. **Code Separation and Modularity**

   - Structure the project into the following directories:
     - `main.py`: The entry point of the application for high-level orchestration.
     - `scraper/`: Contains code for web scraping using Crawl4AI.
     - `models/`: Manages interactions with the LLM, including query handling.
     - `utils/`: Utility functions such as URL validation, error handling, and text processing.
     - `tests/`: Unit and integration tests, ensuring at least 80% test coverage.
   - Avoid mixing concerns in files—each module should handle a single responsibility.

3. **Documentation**

   - Provide module-level docstrings to explain each file’s purpose.
   - Use Google or NumPy-style docstrings for functions and classes.
   - Include detailed inline comments for complex code sections.

4. **Dependency Management**

   - Use a `requirements.txt` file for Python dependencies.
   - Manage environment isolation with `venv` or `poetry`.
   - Keep sensitive data like API keys in a `.env` file and load securely using `python-dotenv`.

5. **Version Control**
   - Use Git for version control with meaningful commit messages.
   - Create branches for features or fixes (e.g., `feature/scraper-setup` or `fix/handle-timeouts`).
   - Include a `.gitignore` file to exclude unnecessary files (e.g., `__pycache__`, `.env`, `dist/`).

---

## Specific Rules for Crawl4AI Integration

1. **Crawl4AI Documentation**

   - Use Crawl4AI documentation to ensure proper API usage and integration.
   - Implement all scraping logic within the `scraper/` module to keep the codebase modular.

2. **Error Handling**

   - Catch and log exceptions like invalid URLs or timeouts.
   - Provide clear, user-friendly error messages.

3. **Configuration Management**
   - Store Crawl4AI configurations (e.g., API keys, scraping depth) in a `.env` file.
   - Include default values for parameters such as scraping depth or rate limits.

---

## Specific Rules for LLM Integration

1. **Query Handling**

   - Abstract LLM interaction in the `models/` module.
   - Sanitize user inputs to prevent injection attacks or malformed requests.

2. **Rate Limiting**

   - Implement rate-limiting logic if the LLM API has usage restrictions.
   - Notify users when limits are approached or exceeded.

3. **Response Validation**
   - Validate and sanitize responses from the LLM before displaying or further processing.

---

## Cursor AI-Specific Rules

1. **AI Suggestions**

   - Configure Cursor AI to prioritize Python best practices for Crawl4AI and LLM integration.
   - Enable AI suggestions for docstring generation and code refactoring.

2. **Code Exclusions**

   - Exclude temporary or autogenerated files (e.g., `__pycache__`, `.env`, `.log`) from AI suggestions.

3. **Documentation Support**
   - Use Cursor AI to generate and refine documentation from Crawl4AI and LLM resources.
   - Leverage AI to automate repetitive coding tasks like boilerplate function creation.

---

## Summary

This configuration ensures the project is developed with Python best practices, modular design, and robust error handling while leveraging AI to improve efficiency. All components, including Crawl4AI and LLM integration, are organized for maintainability and scalability.
